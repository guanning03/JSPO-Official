<!DOCTYPE html>
<html lang="en" class="fontawesome-i2svg-active fontawesome-i2svg-complete">
<head>
  <meta charset="utf-8">
  <meta name="description" content="James-Stein Policy Optimization is a variance-reduced reinforcement learning method for large reasoning models.">
  <meta name="keywords" content="LLM, Reasoning, Reinforcement Learning, Variance Reduction">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Variance-Reduced Reinforcement Learning for Large Reasoning Models via James-Stein Baselines</title>

  <!-- Stylesheets -->
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <!-- Custom fonts & responsive typography -->
  <style>
    /* Import Google Fonts */
    @import url('https://fonts.googleapis.com/css2?family=Playfair+Display:wght@400;700&family=Open+Sans:wght@400;600&family=Cormorant+Garamond:wght@400;500;600&display=swap');

    /* Root defaults (desktop / large screens) */
    html {
      font-family: 'Open Sans', sans-serif;
      font-size: 18px;   /* base size */
      line-height: 1.6;
      background-color: #fafafa;
      color: #333;
    }

    /* Mobile‑first adjustments */
    @media (max-width: 767px) {
      html { font-size: 15px; }            /* down‑scale all rem units */
      h1.publication-title { font-size: 2rem !important; }
      h2.title.is-3       { font-size: 1.5rem !important; }
      .content,
      .publication-authors,
      .publication-links a span,
      .link-block span    { font-size: 1rem; }
      figcaption          { font-size: 0.85rem; }
      pre code            { font-size: 0.9rem; }
      footer.footer       { font-size: 0.85rem; }
    }

    @media (max-width: 480px) {
      html { font-size: 14px; }            /* extra small screens */
    }

    body {
      background-color: white;
      margin: 0;
      padding: 0;
    }

    /* Headings */
    h1.publication-title,
    h2.title.is-3 {
      font-family: 'Playfair Display', serif !important;
    }

    h1.publication-title { font-size: 3rem !important; }
    h2.title.is-3       { font-size: 2rem !important; }

    /* Text blocks */
    .content,
    .publication-authors,
    .publication-links a span,
    .link-block span {
      font-size: 1.125rem;
      font-family: 'Open Sans', sans-serif;
    }

    /* Layout & components */
    .container { max-width: 720px; margin: 0 auto; padding: 0 1rem; }
    .hero .container { max-width: 960px; }
    .hero      { padding: 2.5rem 1rem; background-color: #f0f0f0; }
    .section   { padding: 3rem 0; }

    .teaser-image {
      max-width: 100%;
      height: auto;
      margin: 1.5rem auto;
      display: block;
      border-radius: 8px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }

    figcaption {
      font-size: 0.95rem;
      color: #555;
      margin-top: 0.5rem;
      text-align: center;
      font-style: italic;
      font-family: 'Open Sans', sans-serif;
    }

    pre {
      display: block;
      overflow-x: auto;
      padding: 1rem;
      background: #f5f5f5;
      border-radius: 4px;
      white-space: pre;
      word-break: normal;
    }

    pre code {
      font-family: monospace;
      font-size: 1rem;
      background: #f5f5f5;
      padding: 1rem;
      border-radius: 4px;
      overflow-x: auto;
    }

    footer.footer {
      background-color: #f8f8f8;
      font-size: 0.95rem;
      padding: 2rem 1rem;
    }
  </style>

  <!-- Scripts -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>

  <!-- Hero / Title block -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <div class="columns is-centered">
          <div class="column has-text-centered">

            <!-- Title -->
            <h1 class="title is-1 publication-title">
              Variance-Reduced Reinforcement Learning for Large Reasoning Models via James-Stein Baselines
            </h1>

            <!-- Authors -->
            <p class="is-size-5 publication-authors">
              <span class="author-block"><a href="https://scholar.google.com/citations?user=SU6ooAQAAAAJ">Guanning Zeng</a></span>,
              <span class="author-block"><a href="https://zhaoyizhou1123.github.io">Zhaoyi Zhou</a></span>,
              <span class="author-block"><a href="https://daman1209arora.github.io">Daman Aurora</a></span>,
              <span class="author-block"><a href="https://azanette.com">Andrea Zanette</a></span>
            </p>
            <p class="is-size-5 publication-authors">
              Carnegie Mellon University
            </p>

            <!-- Links -->
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2506.09016" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/Zanette-Labs/speed-rl" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/collections/rqzhang/speed-rl-684a72dfb24ea72540c32fa1" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <svg class="svg-inline--fa fa-database fa-w-14" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="database" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 73.143v45.714C448 159.143 347.667 192 224 192S0 159.143 0 118.857V73.143C0 32.857 100.333 0 224 0s224 32.857 224 73.143zM448 176v102.857C448 319.143 347.667 352 224 352S0 319.143 0 278.857V176c48.125 33.143 136.208 48.572 224 48.572S399.874 209.143 448 176zm0 160v102.857C448 479.143 347.667 512 224 512S0 479.143 0 438.857V336c48.125 33.143 136.208 48.572 224 48.572S399.874 369.143 448 336z"></path></svg>
                  </span>
                  <span>Datasets</span>
                </a>
              </span>
            </div> <!-- /.publication-links -->

            <!-- Teaser sentence -->
            <p class="is-size-4 has-text-centered" style="margin-top: 2rem; font-style: italic; color: #555; font-family: 'Cormorant Garamond', serif;">
              <strong>"Careful interpolation between prompt and batch means for baselines in RLVR reduces the policy gradient variance."</strong>
            </p>

          </div> <!-- /.column -->
        </div> <!-- /.columns -->
      </div> <!-- /.container -->
    </div> <!-- /.hero-body -->
  </section>

  <!-- Main content -->
  <section class="section">
    <div class="container is-max-widescreen">
      <div class="columns is-centered">
        <div class="column is-full-width">
          
          <!-- Overview -->
          <div class="content has-text-justified">
            <p>
              We introduce <strong>James-Stein Shrinkage Baseline</strong>, a baseline for critic‑free, variance‑reduced reinforcement learning in large reasoning models. 
            </p>
            <ul>
              <li><strong>Lower‑variance updates.</strong> A closed‑form, data‑dependent shrinkage coefficient pools signal across the batch when per‑prompt estimates are noisy and backs off when prompts are heterogeneous.</li>
              <li><strong>Drop‑in & compute‑light.</strong> Works out of the box with rule‑based RLVR algorithms such as GRPO, RLOO, REINFORCE++, DAPO, DR.GRPO, GSPO, CISPO, ..., without introducing any additional hyperparameters or computational overhead.</li>
              <li><strong>Better training stability.</strong> Outperforming existing baselines under different tasks, models and rollout budgets.</li>
            </ul>
            <figure>
              <img src="./static/images/teaser_figure.png" alt="Overview of JSPO" class="teaser-image">
              <figcaption><strong>Figure 1:</strong> In critic-free RLVR, we can replace noisy per‑prompt baselines with a James–Stein shrinkage baseline. The shrinkage adapts to within‑prompt noise and across‑prompt heterogeneity, yielding lower‑variance yet unbiased gradients and better, steadier learning.</figcaption>
            </figure>
          </div> <!-- /.content -->
          
          <h2 class="title is-3" id="srt">Introduction</h2>
          <div class="content has-text-justified">
            <p>
              We consider Reinforcement Learning with Verifiable Rewards (RLVR). Let the policy be <span style="white-space:nowrap;">\(\pi_\theta(y \mid x)\)</span>, reward \(r(x,y)\in\mathbb{R}\), and the objective
            </p>
            <p>
              \[
              J(\theta) \;:=\; \mathbb{E}_{x\sim\mathcal{D},\,y\sim\pi_\theta(\cdot\mid x)}\,[\,r(x,y)\,] \, .
              \]
            </p>
            <p>
              At each RL step, we sample \(n\) prompts \(x_1,\dots,x_n\) and, for each prompt \(x_i\), generate \(m\) independent responses \(y_i^1,\dots,y_i^m\) with rewards \(r_i^j := r(x_i,y_i^j)\). Using per‑sample baselines \(b_i^j\), the (critic‑free) batch policy‑gradient estimator is
            </p>
            <p>
              \[
              g(x,Y;\theta)
              \;:=\;
              \frac{1}{n}\sum_{i=1}^{n}\frac{1}{m}\sum_{j=1}^{m}
              \bigl(r_i^j - b_i^j\bigr)\,\nabla_\theta \log \pi_\theta(y_i^j \mid x_i)\,.
              \]
            </p>
            <p>
              This estimator is <em>unbiased</em> whenever each baseline \(b_i^j\) is independent of its paired reward \(r_i^j\); leave‑one‑out constructions (e.g., RLOO‑style) enforce this independence in practice.
            </p>
          
            <!-- <p>
              To measure estimator quality, we track the variance (trace of the covariance) of the vector‑valued gradient:
            </p>
            <p>
              \[
              \mathrm{Var}[g]
              \;:=\;
              \operatorname{Tr}\,\mathrm{Var}\!\big[g(x,Y;\theta)\big]
              \;=\;
              \mathbb{E}\bigl[\;\|\,g(x,Y;\theta)-\nabla_\theta J(\theta)\,\|_2^2\bigr]\,.
              \]
            </p> -->
          
            <p>
              A standard and accurate simplification for baseline estimation \(b(x)\) is to make it close to the <em>value‑function</em> \(\mu(x):=\mathbb{E}_{y\sim\pi_\theta}[r(x,y)]\) (e.g. In all actor-critic methods, the value function is estimated from a critic network which serves as the baseline). For a fixed prompt \(x\),
            </p>
            <p>
              \[
              \mathbb{E}_{y}\!\big[(r(x,y)-b(x))^2\big]
              \;=\;
              \underbrace{\mathrm{Var}_{y}[r(x,y)]}_{\text{irreducible}} \;+\;
              \underbrace{(b(x)-\mu(x))^2}_{\text{baseline MSE}}\,,
              \]
            </p>
            <p>
              so variance reduction of policy gradient estimator reduces to estimating the value function \(\mu(x)\) accurately. In the multi‑prompt setting, the target is the <em>batch</em> MSE over all prompts and rollouts:
            </p>
            <p> \[ \begin{aligned} \mathrm{MSE} &= \frac{1}{mn}\sum_{i=1}^{n}\sum_{j=1}^{m} \mathbb{E}\!\left[\bigl(b_i^j-\mu_i\bigr)^2\right] \\[2pt] &= \frac{1}{mn}\sum_{i=1}^{n}\sum_{j=1}^{m} \mathbb{E}\!\left[\bigl(b_i^j-\mathbb{E}[b_i^j]+\mathbb{E}[b_i^j]-\mu_i\bigr)^2\right] \\[2pt] &= \frac{1}{mn}\sum_{i=1}^{n}\sum_{j=1}^{m} \Bigl(\mathbb{E}\bigl[(b_i^j-\mathbb{E}[b_i^j])^2\bigr] + 2\,\mathbb{E}[b_i^j-\mathbb{E}[b_i^j]]\,(\mathbb{E}[b_i^j]-\mu_i) + (\mathbb{E}[b_i^j]-\mu_i)^2\Bigr) \\[2pt] &= \frac{1}{mn}\sum_{i=1}^{n}\sum_{j=1}^{m} \Bigl(\underbrace{\mathbb{V}\mathrm{ar}[\,b_i^j\,]}_{\text{Variance}} + \underbrace{(\mathbb{E}[b_i^j]-\mu_i)^2}_{\text{Bias}^2}\Bigr). \end{aligned} \] </p>
          
            <p>
              This batch objective reveals a clear <strong>variance–bias trade‑off</strong> in baseline design:
            </p>
            <ul>
              <li><strong>Per‑prompt means (e.g., GRPO/RLOO):</strong> set \(b_i^j\) near the prompt’s empirical mean. This minimizes bias to \(\mu_i\) but can be <em>high variance</em> when \(m\) is small.</li>
              <li><strong>Batch/global means (e.g., REINFORCE++):</strong> average across prompts to reduce variance, but this introduces <em>bias</em> when prompts’ true values \(\mu_i\) differ.</li>
            </ul>
          
            <p>
              Because we must estimate <em>many</em> prompt values simultaneously, neither extreme is optimal in batch MSE. The James–Stein perspective suggests <em>shrinking</em> each prompt’s baseline toward the batch mean by a data‑dependent amount, balancing variance reduction and bias while preserving unbiased gradients via leave‑one‑out independence.
            </p>
          </div>
          
          <!-- James–Stein Policy Optimization (JSPO): Algorithm Design -->
          <h2 class="title is-3" id="algo">James–Stein Policy Optimization (JSPO): Algorithm Design</h2>
          <div class="content has-text-justified">
            <p>
              Based on the Stein's paradox in statistics, <strong>JSPO</strong> builds a lower-variance baseline by <em>shrinking</em> each prompt’s leave‑one‑out mean reward toward a leave-one-out (LOO) batch mean, with a data‑dependent coefficient. For prompt \(x_i\) (\(i=1,\dots,n\)) with \(m\) rollouts \(r_i^1,\dots,r_i^m\), define the LOO prompt mean and the LOO batch mean as
            </p>
            <p>
              \[
              \hat{\mu}_{i}^{-j} \;=\; \frac{1}{m-1}\!\!\sum_{j'\neq j} r_i^{\,j'}, 
              \qquad
              \hat{\mu}_{-i} \;=\; \frac{1}{n-1}\!\!\sum_{k\neq i} \hat{\mu}_k,
              \quad
              \hat{\mu}_k \;=\; \frac{1}{m}\sum_{j=1}^{m} r_k^{\,j}.
              \]
            </p>
            <p>
              The JSPO baseline for sample \((i,j)\) is the adaptive combination between the LOO prompt mean and the LOO batch mean:
            </p>
            <p>
              \[
              b_{ij}^{\text{JSPO}} \;=\; (1-\hat{\lambda}_i)\,\hat{\mu}_{i}^{-j} \;+\; \hat{\lambda}_i\,\hat{\mu}_{-i},
              \]
              where the shrinkage coefficient \(\hat{\lambda}_i\) is chosen to minimize the batch MSE of value estimates. The optimal shrinkage coefficient is given by:
            </p>
            <p>
              \[
              (\lambda_i^\ast)\;=\;\frac{n-1}{n}\cdot \frac{v^2}{\,s^2+v^2\,},
              \qquad
              v^2 \;=\; \frac{1}{m-1}\,\mathbb{E}_{x}\!\big[\sigma^2(x)\big],\quad
              s^2 \;=\; \mathrm{Var}_{x}\!\big[\mu(x)\big],
              \]
              with \(\sigma^2(x)=\mathrm{Var}_{y\sim\pi_\theta(\cdot|x)}[r(x,y)]\) and \(\mu(x)=\mathbb{E}_{y\sim\pi_\theta(\cdot|x)}[r(x,y)]\).
            </p>
            <p>
              In addition, we estimate the components from the current batch using plug‑in statistics:
            </p>
            <p>
              \[
              \hat{v}_{-i} \;=\; \frac{1}{n-1}\sum_{k\neq i}\left[\frac{1}{m(m-1)}\sum_{j=1}^{m}\big(r_k^{\,j}-\hat{\mu}_k\big)^2\right],
              \qquad
              \hat{s}_{-i} \;=\; \frac{1}{n-1}\sum_{k\neq i}\big(\hat{\mu}_k-\hat{\mu}_{-i}\big)^2,
              \]
              yielding the per‑prompt shrinkage coefficient
              \[
              \hat{\lambda}_i \;=\; \frac{n-1}{n}\cdot \frac{\hat{v}_{-i}}{\,\hat{v}_{-i}+\hat{s}_{-i}\,}.
              \]
              Substituting into the baseline gives \(b_{ij}^{\text{JSPO}}\) above. 
            </p>
            <p>
              when \(v^2 \gg s^2\) (e.g. few rollouts, high within‑prompt noise), \(\hat{\lambda}_i\) is large and our James-Stein baseline adaptively leaning towards the batch mean, similar to REINFORCE++; when \(s^2 \gg v^2\) (e.g. sufficient rollouts, heterogeneous training data), \(\hat{\lambda}_i \approx 0\) and the James-Stein baseline reduces to the prompt mean, similar to GRPO. Under arbitrary conditions, the James-Stein baseline is theoretically guaranteed to be a more accurate compared with other baselines.
            </p>
          </div>          


          <!-- Experiments -->
          <h2 class="title is-3" id="experiments">Experiments</h2>
          <div class="content has-text-justified">
            <p>
              We demonstrate SPEED's efficacy by comparing the <strong>wall-clock time</strong> to achieve certain target performance for baseline RL algorithms and for SPEED variants.
            </p>
            <ul>
              <li>Baseline: <a href="https://arxiv.org/abs/2402.14740" target="_blank" rel="noopener noreferrer">RLOO</a> and <a href="https://arxiv.org/abs/2503.14476" target="_blank" rel="noopener noreferrer">DAPO</a>. We compare RLOO vs SPEED-RLOO, and DAPO vs SPEED-DAPO.</li>
              <li>Training Dataset: <a href="https://huggingface.co/datasets/agentica-org/DeepScaleR-Preview-Dataset" target="_blank" rel="noopener noreferrer">DeepScaleR</a>, <a href="https://huggingface.co/datasets/AI-MO/NuminaMath-TIR" target="_blank" rel="noopener noreferrer">NuminaMath</a>, and <a href="https://huggingface.co/datasets/BytedTsinghua-SIA/DAPO-Math-17k" target="_blank" rel="noopener noreferrer">DAPO-17k</a> (1k held-out set excluded)</li>
              <li>Benchmarks: 1k held-out set from DAPO-17k, MATH500, AMC23, AIME2024, and AIME 2025. See <a href="https://huggingface.co/collections/rqzhang/speed-rl-684a72dfb24ea72540c32fa1" target="_blank" rel="noopener noreferrer">here</a>.</li>
            </ul>
            <p>
              Empirically, SPEED variants substantially accelerate the training compared to the baseline methods by <strong>2x - 6x times</strong>. For detailed empirical results, please refer to our full paper.
            </p>
            <figure>
              <img src="./static/images/main_figure.png" alt="Main experimental figure" class="teaser-image">
              <figcaption><strong>Figure 3:</strong> Validation accuracy on various mathematical reasoning benchmarks for SPEED variants of RL algorithms and base RL algorithms. Top: RLOO versus SPEED-RLOO; bottom: DAPO versus SPEED-DAPO. The initial model used is Qwen2.5-Math-7B, trained on the DeepScaleR dataset.</figcaption>
            </figure>
          </div> <!-- /.content -->


          <h2 class="title is-3" id="analysis">Analysis</h2>
          <div class="content has-text-justified">
            <p>
              We focus on providing insights into two key training dynamics that reveal the advantage of JSPO: namely, the <strong>adaptive shrinkage</strong> of the James–Stein coefficient and the <strong>reduced variance</strong> of the policy
              gradient.
            </p>
            <p>
              <strong>Adaptive Shrinkage.</strong> The shrinkage coefficient \(\lambda_i\) adapts to number of rollouts, heterogeneity of prompts, and the training progress.
            </p>
            <figure>
              <img src="./static/images/adaptive_shrinkage.png" alt="Adaptive shrinkage coefficient over training and rollout counts" class="teaser-image" style="width: 50%;">
              <figcaption>
                <strong>Figure 2:</strong> Moving‑average shrinkage coefficient \(\lambda\) during training with 2/4/8 rollouts.
                \(\lambda\) decreases with more rollouts and decays over time as the policy’s within‑prompt variance shrinks,
                illustrating JSPO’s data‑driven adaptation. 
              </figcaption>
            </figure>

            <p>
              <strong>Variance Reduction.</strong> JSPO’s goal is to lower the variance of the vector‑valued policy‑gradient
              estimator \(g(x,Y;\theta)\). During training, we track the (unbiased) scalar summary \(\mathrm{Var}[g]\) via micro‑batch
              gradients:
            </p>
            <p>
              \[
                \widehat{\mathrm{Var}}(g)
                \;=\;\frac{1}{m}\cdot\frac{1}{m-1}\sum_{i=1}^{m}\bigl\|g_i-\bar g\bigr\|_2^2
                \;=\;\frac{1}{m}\cdot\frac{1}{m-1}\!\left(\sum_{i=1}^{m}\|g_i\|_2^2 - m\,\|\bar g\|_2^2\right),
                \quad \bar g=\frac{1}{m}\sum_{i=1}^{m} g_i.
              \]
              Empirically, JSPO reduces the running average of gradient variance relative to RLOO by
              <strong>11.2%</strong>, <strong>17.4%</strong>, <strong>31.6%</strong>, and <strong>67.1%</strong> across different models and setups. 
            </p>

            <figure>
              <img src="./static/images/variance_reduction.png" alt="Estimated policy‑gradient variance during training: JSPO vs. RLOO" class="teaser-image", style="width: 90%;">
              <figcaption>
                <strong>Figure 3:</strong> Estimated variance of the policy gradient during training (lower is better). With the
                James–Stein baseline, variance consistently drops across models and tasks, yielding steadier RL updates.
              </figcaption>
            </figure>

            <h2 class="title is-3" id="srt">Citation</h2>
            <p>If you find this work useful, please cite it as follows:</p>

            <pre class="box" style="white-space: pre-wrap; word-break: break-word; overflow-x: auto; font-family: monospace; font-size: 0.9rem;">
              @misc{zhang2025speedrlfastertrainingreasoning,
                title={SPEED-RL: Faster Training of Reasoning Models via Online Curriculum Learning}, 
                author={Ruiqi Zhang and Daman Arora and Song Mei and Andrea Zanette},
                year={2025},
                eprint={2506.09016},
                archivePrefix={arXiv},
                primaryClass={cs.LG},
                url={https://arxiv.org/abs/2506.09016}
              }
              </pre>
              


  <br><br>
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <p>
        Corresponding Authors: <a href="mailto:zanette@cmu.edu">Andrea Zanette</a>, <a href="mailto:zgn21@mails.tsinghua.edu.cn">Guanning Zeng</a>.<br>
      </p>
    </div>
  </div>
</footer>

</body>
</html>
