<!DOCTYPE html>
<html lang="en" class="fontawesome-i2svg-active fontawesome-i2svg-complete">
<head>
  <meta charset="utf-8">
  <meta name="description" content="James-Stein Policy Optimization is a variance-reduced reinforcement learning method for large reasoning models.">
  <meta name="keywords" content="LLM, Reasoning, Reinforcement Learning, Variance Reduction">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Shrinking the Variance: Shrinkage Baselines for Reinforcement Learning with Verifiable Rewards</title>

  <!-- Stylesheets -->
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <!-- Custom fonts & responsive typography -->
  <style>
    /* Import Google Fonts */
    @import url('https://fonts.googleapis.com/css2?family=Playfair+Display:wght@400;700&family=Open+Sans:wght@400;600&family=Source+Serif+4:ital,wght@0,400;0,600;1,400&display=swap');

    /* Root defaults (desktop / large screens) */
    html {
      font-family: 'Open Sans', sans-serif;
      font-size: 18px;   /* base size */
      line-height: 1.6;
      background-color: #fafafa;
      color: #333;
    }

    /* Mobile‑first adjustments */
    @media (max-width: 767px) {
      html { font-size: 15px; }            /* down‑scale all rem units */
      h1.publication-title { font-size: 2rem !important; }
      h2.title.is-3       { font-size: 1.5rem !important; }
      .content,
      .publication-authors,
      .publication-links a span,
      .link-block span    { font-size: 1rem; }
      figcaption          { font-size: 0.85rem; }
      pre code            { font-size: 0.9rem; }
      footer.footer       { font-size: 0.85rem; }
    }

    @media (max-width: 480px) {
      html { font-size: 14px; }            /* extra small screens */
    }

    body {
      background-color: white;
      margin: 0;
      padding: 0;
    }

    /* Headings */
    h1.publication-title,
    h2.title.is-3 {
      font-family: 'Playfair Display', serif !important;
    }

    h1.publication-title { font-size: 3rem !important; }
    h2.title.is-3       { font-size: 2rem !important; }

    /* Text blocks */
    .content,
    .publication-authors,
    .publication-links a span,
    .link-block span {
      font-size: 1.125rem;
      font-family: 'Open Sans', sans-serif;
    }

    /* Layout & components */
    .container { max-width: 720px; margin: 0 auto; padding: 0 1rem; }
    .hero .container { max-width: 960px; }
    .hero      { padding: 2.5rem 1rem; background-color: #f0f0f0; }
    .section   { padding: 3rem 0; }

    .teaser-image {
      max-width: 100%;
      height: auto;
      margin: 1.5rem auto;
      display: block;
      border-radius: 8px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }

    figcaption {
      font-size: 0.95rem;
      color: #555;
      margin-top: 0.5rem;
      text-align: center;
      font-style: italic;
      font-family: 'Open Sans', sans-serif;
    }

    pre {
      display: block;
      overflow-x: auto;
      padding: 1rem;
      background: #f5f5f5;
      border-radius: 4px;
      white-space: pre;
      word-break: normal;
    }

    pre code {
      font-family: monospace;
      font-size: 1rem;
      background: #f5f5f5;
      padding: 1rem;
      border-radius: 4px;
      overflow-x: auto;
    }

    footer.footer {
      background-color: #f8f8f8;
      font-size: 0.95rem;
      padding: 2rem 1rem;
    }
  </style>

  <!-- Scripts -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<!-- MathJax v3 (LaTeX rendering) -->
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<style>
  /* 轻量 tooltip：把说明放到 data-note，悬停时浮出 */
  .hover-note {
    position: relative;
    cursor: help;
    border-bottom: 1px dotted #999;
    display: inline-block;
    line-height: 1;
    padding: 0 0.2em;
    color: #666;
    font-size: 0.95em;
  }
  .hover-note::after {
    content: attr(data-note);
    position: absolute;
    left: 0;
    bottom: 125%;
    transform: translateY(-4px);
    background: #111;
    color: #fff;
    padding: 8px 10px;
    border-radius: 6px;
    box-shadow: 0 6px 18px rgba(0,0,0,0.2);
    white-space: normal;
    max-width: 420px;
    font-size: 0.9em;
    line-height: 1.4;
    opacity: 0;
    pointer-events: none;
    transition: opacity .15s ease, transform .15s ease;
    z-index: 1000;
  }
  .hover-note::before {
    content: '';
    position: absolute;
    left: 10px;
    bottom: 125%;
    transform: translateY(6px);
    border: 6px solid transparent;
    border-top-color: #111;
    opacity: 0;
    transition: opacity .15s ease;
    z-index: 1001;
  }
  .hover-note:hover::after,
  .hover-note:hover::before {
    opacity: 1;
    transform: translateY(0);
  }

  /* 可选：长公式块与提示图标的行内排版 */
  .formula-wrap {
    display: inline-flex;
    align-items: center;
    gap: 8px;
  }
</style>

</head>

<body>

  <!-- Hero / Title block -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <div class="columns is-centered">
          <div class="column has-text-centered">

            <!-- Title -->
            <h1 class="title is-1 publication-title">
              Shrinking the Variance: Shrinkage Baselines for Reinforcement Learning with Verifiable Rewards
            </h1>

            <!-- Authors -->
            <p class="is-size-5 publication-authors">
              <span class="author-block"><a href="https://scholar.google.com/citations?user=SU6ooAQAAAAJ">Guanning Zeng</a></span>,
              <span class="author-block"><a href="https://zhaoyizhou1123.github.io">Zhaoyi Zhou</a></span>,
              <span class="author-block"><a href="https://daman1209arora.github.io">Daman Arora</a></span>,
              <span class="author-block"><a href="https://azanette.com">Andrea Zanette</a></span>
            </p>
            <p class="is-size-5 publication-authors">
              Carnegie Mellon University
            </p>

            <!-- Links -->
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2511.03710" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/Zanette-Labs/speed-rl" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="https://huggingface.co/collections/rqzhang/speed-rl-684a72dfb24ea72540c32fa1" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <svg class="svg-inline--fa fa-database fa-w-14" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="database" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 73.143v45.714C448 159.143 347.667 192 224 192S0 159.143 0 118.857V73.143C0 32.857 100.333 0 224 0s224 32.857 224 73.143zM448 176v102.857C448 319.143 347.667 352 224 352S0 319.143 0 278.857V176c48.125 33.143 136.208 48.572 224 48.572S399.874 209.143 448 176zm0 160v102.857C448 479.143 347.667 512 224 512S0 479.143 0 438.857V336c48.125 33.143 136.208 48.572 224 48.572S399.874 369.143 448 336z"></path></svg>
                  </span>
                  <span>Datasets</span>
                </a>
              </span> -->
            </div> <!-- /.publication-links -->

            <!-- Teaser sentence -->
            <p class="is-size-4 has-text-centered" style="margin-top: 2rem; font-style: italic; color: #555; font-family: 'Source Serif 4', Georgia, serif;">
              "Careful interpolation between prompt and batch means for baselines in RLVR reduces the policy gradient variance."
            </p>

          </div> <!-- /.column -->
        </div> <!-- /.columns -->
      </div> <!-- /.container -->
    </div> <!-- /.hero-body -->
  </section>

  <!-- Main content -->
  <section class="section">
    <div class="container is-max-widescreen">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <!-- Overview -->
          <div class="content has-text-justified">
            <p>
              We introduce <strong>James-Stein Shrinkage Baseline</strong>, a baseline for critic‑free, variance‑reduced reinforcement learning with verifiable rewards (RLVR) in large reasoning models. 
            </p>
            <ul>
              <li><strong>Lower‑variance updates.</strong> A closed‑form, data‑dependent shrinkage coefficient pools signal across the batch when per‑prompt estimates are noisy and backs off when prompts are heterogeneous.</li>
              <li><strong>Drop‑in & compute‑light.</strong> Works out of the box with rule‑based RLVR algorithms such as GRPO, RLOO, REINFORCE++, DAPO, DR.GRPO, GSPO and CISPO, without introducing any additional hyperparameters or computational overhead.</li>
              <li><strong>Better training stability.</strong> Outperforming existing baselines under different tasks, models and rollout budgets.</li>
            </ul>
            <figure>
              <img src="./static/images/teaser_01.png" alt="Overview of James-Stein Shrinkage Baseline" class="teaser-image">
              <figcaption><strong>Figure 1:</strong> In critic-free RLVR, we can refine the common choice of using noisy prompt-mean baselines by using a James-Stein shrinkage baseline. The shrinkage adapts to within‑prompt noise and across‑prompt heterogeneity, yielding lower‑variance yet unbiased gradients and better, steadier training.</figcaption>
            </figure>
          </div> <!-- /.content -->

        <h2 class="title is-3" id="srt">Introduction</h2>
        <div class="content has-text-justified">
          
          <p>
            We consider Reinforcement Learning with Verifiable Rewards (RLVR) in large reasoning models. In this setting, we aim to optimize a language model policy <span style="white-space:nowrap;">\(\pi_\theta(y \mid x)\)</span>, where \(x\) is a prompt (e.g., a math problem) and \(y\) is the model's response (e.g., a reasoning chain and answer). The quality of each response is measured by a verifiable reward function \(r(x,y)\in\mathbb{R}\), which can be computed using, for instance, automatic graders that check correctness.
          </p>
          
          <p>
            At each reinforcement learning iteration, we follow a standard sampling procedure: we first sample a batch of \(n\) prompts \(x_1,\dots,x_n\) from the training distribution. Then, for each prompt \(x_i\), we generate \(m\) independent responses \(y_i^1,\dots,y_i^m\) by sampling from the current policy. Each response \(y_i^j\) receives a corresponding reward \(r_i^j := r(x_i,y_i^j)\). This produces a total of \(n \times m\) prompt-response pairs with associated rewards per iteration. The surrogate objective in the well-known <a href="https://arxiv.org/pdf/2402.03300" target="_blank"><u>Group Relative Policy Optimization (GRPO) algorithm</u></a> can be written as:
          </p>

          <p style="text-align: center;">
            \[
            \mathcal{J}_{\mathrm{GRPO}}(\theta)
            := \frac{1}{n}\sum_{i=1}^{n}\frac{1}{m}\sum_{j=1}^{m}\frac{1}{\lvert y_i^{\,j}\rvert}\sum_{t=1}^{\lvert y_i^{\,j}\rvert}
            \min\!\Big(\,\rho_{i,t}^{\,j}\,A_i^{\,j},\;
            \operatorname{clip}\!\big(\rho_{i,t}^{\,j},\,1-\epsilon,\,1+\epsilon\big)\,A_i^{\,j}\Big) \, ,
            \]
          </p>
          
          <p>
            where \(\lvert y_i^{\,j}\rvert\) denotes the token length of response \(y_i^{\,j}\), and the objective aggregates contributions across all prompts, responses, and tokens. Here, \(\rho_{i,t}^{\,j}\) is the importance sampling ratio at token position \(t\):
          </p>

          <p style="text-align: center;">
            \[
            \rho_{i,t}^{\,j}
            := \frac{\pi_\theta\!\big(y_{i,t}^{\,j}\mid x_i,\,y_{i,1{:}t-1}^{\,j}\big)}
            {\pi_{\theta_{\mathrm{old}}}\!\big(y_{i,t}^{\,j}\mid x_i,\,y_{i,1{:}t-1}^{\,j}\big)} \, ,
            \]
          </p>

          <p>
            which measures the ratio between the current policy \(\pi_\theta\) and the old policy \(\pi_{\theta_{\mathrm{old}}}\) for generating token \(y_{i,t}^{\,j}\). Meanwhile, \(A_i^{\,j}\) is the <strong>advantage</strong>, defined as:
          </p>

          <p style="text-align: center;">
            \[
            A_i^{\,j} := r_i^{\,j}{\color{red} - b_i^{\,j}}\, ,
            \]
          </p>

          <p>
            where \(r_i^{\,j}\) is the observed reward for response \(y_i^{\,j}\), and \(b_i^{\,j}\) is the <strong>baseline</strong>. 
            Note that in standard GRPO implementations, the advantage \(A_i^{\,j}\) is often further normalized by dividing by the empirical standard deviation across all advantages in the batch. We ignore this normalization step during analysis, as <a href="https://arxiv.org/pdf/2510.13786" target="_blank"><u>recent empirical work</u></a> has demonstrated that both the normalized and unnormalized versions exhibit similar scaling behavior in terms of final performance and training dynamics.
          </p>

          <p>
            We focus on <a href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf" target="_blank"><u>baseline</u></a> \(b^j_i\), a commonly used control variate in deep reinforcement learning. Subtracting the rewards \(r_i^j\) by the baseline reduces the variance of the policy-gradient estimate, which is very essential to stabilize training. For example, in GRPO, the baseline is the empirical prompt mean. Below is a leave-one-out estimation:
          </p>

          <p class="has-text-centered">
            \(b_i^j = \frac{1}{m-1} \sum_{k=1}^{m} r_i^k, \; k\neq j\)
          </p>

          <!-- 结尾问题，按指定字体与样式 -->
          <p>
            However, is the empirical prompt mean the theoretically optimal baseline for RLVR?
          </p>
        </div>

          <h2 class="title is-3" id="srt">A Bias-Variance Tradeoff</h2>
          <div class="content has-text-justified">          
            <p>
              The baseline is a essential component in RLVR to stablize training, as experimental results show substantial performance degradation without it. A golden standard for baseline estimation \(b(x)\) is to make it close to the <em>value‑function</em> \(\mu(x):=\mathbb{E}_{y\sim\pi_\theta}[r(x,y)]\). (For example, in all actor-critic methods, the value function is estimated from a critic network which serves as the baseline. )
            </p>
            <p>
              For a fixed prompt \(x\),
              \[
              \mathbb{E}_{y}\!\big[(r(x,y)-b(x))^2\big]
              \;=\;
              \underbrace{\mathrm{Var}_{y}[r(x,y)]}_{\text{irreducible}} \;+\;
              \underbrace{(b(x)-\mu(x))^2}_{\text{baseline MSE}}\,,
              \]
            </p>
            <p>
              so variance reduction of policy gradient estimator reduces to estimating the value function \(\mu(x)\) accurately. Therefore, for different kinds of baselines, we would like to see how close the estimation is compared with the value function. Specifically, consider two different ways, empirical prompt mean (which is most commonly used in RLVR such as GRPO) and empirical batch mean (some algorithms, such as REINFORCE++ adopts this)
            </p>
            <p style="text-align: center;">
              <strong>Prompt Mean:</strong> 
              \(\displaystyle b_i^j = \hat{\mu}_i^{-j} = \frac{1}{m-1} \sum_{\substack{k=1\\k\neq j}}^{m} r_i^k\)
              <br><br>
              <strong>Batch Mean:</strong> 
              \(\displaystyle b_i^j = \hat{\mu} = \frac{1}{nm} \sum_{i=1}^{n} \sum_{j=1}^{m} r_i^j\)
            </p>
            <p>
              We computed the mean squared error (MSE) between these baselines and the value function (which could be estimated by rollout for many times) during RLVR training and in different number of rollouts \(n\). As shown in the figure below, the MSE of prompt mean is lower in most cases, while batch mean is better when the number of rollouts is small or in early training stages. So how can we create a baseline estimator which enjoys lowest MSE with the ground truth value function in all scenarios?
            </p>
            <p>
              To answer this question, we first write down the target: in the multi‑prompt setting, it is the <em>batch</em> MSE over all prompts and rollouts:
            </p>
            <p> \[ \begin{aligned} \mathrm{MSE} &= \frac{1}{mn}\sum_{i=1}^{n}\sum_{j=1}^{m} \mathbb{E}\!\left[\bigl(b_i^j-\mu_i\bigr)^2\right] \\[2pt] &= \frac{1}{mn}\sum_{i=1}^{n}\sum_{j=1}^{m} \mathbb{E}\!\left[\bigl(b_i^j-\mathbb{E}[b_i^j]+\mathbb{E}[b_i^j]-\mu_i\bigr)^2\right] \\[2pt] &= \frac{1}{mn}\sum_{i=1}^{n}\sum_{j=1}^{m} \Bigl(\mathbb{E}\bigl[(b_i^j-\mathbb{E}[b_i^j])^2\bigr] + 2\,\mathbb{E}[b_i^j-\mathbb{E}[b_i^j]]\,(\mathbb{E}[b_i^j]-\mu_i) + (\mathbb{E}[b_i^j]-\mu_i)^2\Bigr) \\[2pt] &= \frac{1}{mn}\sum_{i=1}^{n}\sum_{j=1}^{m} \Bigl(\underbrace{\mathbb{V}\mathrm{ar}[\,b_i^j\,]}_{\text{Variance}} + \underbrace{(\mathbb{E}[b_i^j]-\mu_i)^2}_{\text{Bias}^2}\Bigr). \end{aligned} \] </p>
            <p>
              This batch objective reveals a clear <strong>variance–bias trade‑off</strong> in baseline design:
            </p>
            <ul>
              <li><strong>Prompt means (e.g., GRPO/RLOO):</strong> set \(b_i^j\) near the prompt’s empirical mean, which is the usual choice for most RLVR algorithms. This minimizes bias to \(\mu_i\) but can be <em>high variance</em> when \(m\) is small.</li>
              <li><strong>Batch means (e.g., REINFORCE++):</strong> is another possible choice. It average across prompts to reduce variance, but this introduces <em>bias</em> when prompts’ true values \(\mu_i\) differ.</li>
            </ul>
          
            <p>
              Because we must estimate <em>many</em> prompt values simultaneously, neither extreme is optimal. 
            </p>              
          </div>
          
          <!-- James–Stein Policy Optimization (JSPO): Algorithm Design -->
          <h2 class="title is-3" id="algo">Shrinkage Baseline</h2>
          <div class="content has-text-justified">
            <p>
              Based on the solution to the famous Stein's paradox in statistics, an adaptive interpolation between prompt mean and batch mean form a strictly better estimation during simultaneous estimation of multiple value functions.
            </p>
            <p>
              \[
              b_{ij}^{\text{JS}} \;=\; (1-\hat{\lambda}_i)\,\hat{\mu}_{i}^{-j} \;+\; \hat{\lambda}_i\,\hat{\mu}_{-i},
              \]
            </p>
            <p>
              the \(\hat{\mu}_{i}^{-j}\) and \(\hat{\mu}_{-i}\) are the empirical prompt mean and batch mean, respectively, and the \(\hat{\lambda}_i\) is called the shrinkage coefficient. 
            </p>
            <p>
              \[
              \hat{\mu}_{i}^{-j} \;=\; \frac{1}{m-1}\!\!\sum_{j'\neq j} r_i^{\,j'}, 
              \qquad
              \hat{\mu}_{-i} \;=\; \frac{1}{n-1}\!\!\sum_{k\neq i} \hat{\mu}_k,
              \quad
              \hat{\mu}_k \;=\; \frac{1}{m}\sum_{j=1}^{m} r_k^{\,j}.
              \]
            </p>
            <p>
              Through derivations, we have the optimal shrinkage coefficient as below:
            </p>
            <p>
              \[
              (\lambda_i^\ast)\;=\;\frac{n-1}{n}\cdot \frac{v^2}{\,s^2+v^2\,},
              \qquad
              v^2 \;=\; \frac{1}{m-1}\,\mathbb{E}_{x}\!\big[\sigma^2(x)\big],\quad
              s^2 \;=\; \mathrm{Var}_{x}\!\big[\mu(x)\big],
              \]
              with \(\sigma^2(x)=\mathrm{Var}_{y\sim\pi_\theta(\cdot|x)}[r(x,y)]\) and \(\mu(x)=\mathbb{E}_{y\sim\pi_\theta(\cdot|x)}[r(x,y)]\).
            </p>
            <p>
              In practice, we can estimate both components from the current batch using plug‑in statistics:
            </p>
            <p>
              \[
              \hat{v}_{-i} \;=\; \frac{1}{n-1}\sum_{k\neq i}\left[\frac{1}{m(m-1)}\sum_{j=1}^{m}\big(r_k^{\,j}-\hat{\mu}_k\big)^2\right],
              \qquad
              \hat{s}_{-i} \;=\; \frac{1}{n-1}\sum_{k\neq i}\big(\hat{\mu}_k-\hat{\mu}_{-i}\big)^2,
              \]
              yielding the per‑prompt shrinkage coefficient
              \[
              \hat{\lambda}_i \;=\; \frac{n-1}{n}\cdot \frac{\hat{v}_{-i}}{\,\hat{v}_{-i}+\hat{s}_{-i}\,}.
              \]
              Substituting into the baseline gives \(b_{ij}^{\text{JS}}\) above. 
            </p>
            <p>
              when \(v^2 \gg s^2\) (e.g. few rollouts, high within‑prompt noise), \(\hat{\lambda}_i\) is large and our James-Stein baseline adaptively leaning towards the batch mean, similar to REINFORCE++; when \(s^2 \gg v^2\) (e.g. sufficient rollouts, heterogeneous training data), \(\hat{\lambda}_i \approx 0\) and the James-Stein baseline reduces to the prompt mean, similar to GRPO. Under arbitrary conditions, the James-Stein baseline is theoretically guaranteed to be a more accurate compared with other baselines.
            </p>
          </div>          


          <!-- Experiments -->
          <h2 class="title is-3" id="experiments">Experiments and Analysis</h2>
          <div class="content has-text-justified">
            <p>
              We evaluate the performance of the James-Stein baseline on different tasks (Math Problems, Knights-and-Knaves, Maze, Countdown) and models (Qwen3-4B-Base, Qwen2.5-1.5B-Instruct, Qwen2.5-7B-Instruct, Ministral-8B-Instruct). We compare the performance of the James-Stein baseline with the empirical prompt mean without shrinkage. The JS baseline achieves lower variance and better performance than the empirical prompt mean, with up to 4.3% in math reasoning and up to 15.1% on logic puzzle reasoning. For detailed empirical results, please refer to our full paper.
            </p>
            <figure>
              <img src="./static/images/qwen3-4b_01.png" alt="Math Reasoning Performance" class="teaser-image">
              <figcaption><strong>Figure 2:</strong> Math Reasoning Performance. Qwen3-4B-Base trained on DAPO17k dataset. </figcaption>
            </figure>            
            <figure>
              <img src="./static/images/knk_plots_01.png" alt="Logic Puzzle (Knights-and-Knaves) Reasoning Performance" class="teaser-image">
              <figcaption><strong>Figure 3:</strong> Logic Puzzle (Knights-and-Knaves) Reasoning Performance. Qwen2.5-1.5B-Instruct trained on Knight-Knave dataset with mixed difficulty levels. </figcaption>
            </figure>    

            <p>
              We observe two key training dynamics that reveal the advantage of JS baseline: namely, the <strong>adaptive shrinkage</strong> of the JS coefficient, and the <strong>reduced variance</strong> of the policy gradient.
            </p>
            <p>
              <strong>Adaptive Shrinkage.</strong> The shrinkage coefficient \(\lambda_i\) is adaptively chosen to minimize the batch MSE of value estimates. In the figure below, we plot the shrinkage coefficient \(\lambda_i\) during training under different numbers of rollout per prompt. We can see that the shrinkage coefficient is smaller when we choose a larger number of rollouts, or when our training goes into later stages. These results aligns with the intuition that shrinkage coefficient measures the extent of benefit that using batch mean could bring. If the number of rollouts is already rather large, or the policy becomes very deterministic in final training stages, than the benefit of taking cross-prompt information into account is smaller, and we should use a smaller shrinkage coefficient when computing the baseline. 
            </p>
            <figure>
              <img src="./static/images/js_lambda_ema_01.png" alt="Adaptive shrinkage of the JS coefficient" class="teaser-image", style="width: 40%;">
              <figcaption>
                <strong>Figure 3:</strong> Adaptive shrinkage of the JS coefficient during training. The shrinkage coefficient decreases throughout training, and is smaller when there is a larger number of rollouts.
              </figcaption>
            </figure>
            <p>
              <strong>Variance Reduction.</strong> The goal of using JS baseline is to lower the variance of the vector‑valued policy‑gradient
              estimator \(g(x,Y;\theta)\). During training, we estimate the variance of the policy gradient \(\mathrm{Var}[g]\) based on multiple micro‑batch
              gradients:
            </p>
            <p>
              \[
                \widehat{\mathrm{Var}}(g)
                \;=\;\frac{1}{m}\cdot\frac{1}{m-1}\sum_{i=1}^{m}\bigl\|g_i-\bar g\bigr\|_2^2
                \;=\;\frac{1}{m}\cdot\frac{1}{m-1}\!\left(\sum_{i=1}^{m}\|g_i\|_2^2 - m\,\|\bar g\|_2^2\right),
                \quad \bar g=\frac{1}{m}\sum_{i=1}^{m} g_i.
              \]
              Empirically, JS baseline reduces the running average of gradient variance relative to prompt mean baseline by up to 67.1% across different models and setups. 
            </p>

            <figure>
              <img src="./static/images/variance_reduction_01.png" alt="Estimated policy‑gradient variance during training: JSPO vs. RLOO" class="teaser-image", style="width: 70%;">
              <figcaption>
                <strong>Figure 4:</strong> Estimated variance of the policy gradient during training (lower is better). With the
                James–Stein baseline, variance consistently drops across models and tasks, yielding steadier RL updates.
              </figcaption>
            </figure>

            <p>
              Overall, the theorical and empirical analysis demonstrate the effectiveness of a shrinkage baseline in reducing the variance of the policy gradient, which improves the training stability and accuracy for RLVR on reasoning LLMs.
            </p>

            <h2 class="title is-3" id="srt">Citation</h2>
            <p>If you find this work useful, please cite it as follows:</p>

            <pre class="box" style="white-space: pre-wrap; word-break: break-word; overflow-x: auto; font-family: monospace; font-size: 0.9rem;">
              @article{zeng2025shrinking,
                title={Shrinking the Variance: Shrinkage Baselines for Reinforcement Learning with Verifiable Rewards},
                author={Zeng, Guanning and Zhou, Zhaoyi and Arora, Daman and Zanette, Andrea},
                journal={arXiv preprint arXiv:2511.03710},
                year={2025}
              }
              </pre>
  <br><br>
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <p>
        Corresponding Authors: <a href="mailto:zanette@cmu.edu">Andrea Zanette</a>, <a href="mailto:zgn21@mails.tsinghua.edu.cn">Guanning Zeng</a>.<br>
      </p>
    </div>
  </div>
</footer>

</body>
</html>
