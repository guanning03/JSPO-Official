<!DOCTYPE html>
<html lang="en" class="fontawesome-i2svg-active fontawesome-i2svg-complete">
<head>
  <meta charset="utf-8">
  <meta name="description" content="James-Stein Policy Optimization is a variance-reduced reinforcement learning method for large reasoning models.">
  <meta name="keywords" content="LLM, Reasoning, Reinforcement Learning, Variance Reduction">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Shrinking the Variance: Shrinkage Baselines for Reinforcement Learning with Verifiable Rewards</title>

  <!-- Stylesheets -->
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <!-- Custom fonts & responsive typography -->
  <style>
    /* Import Google Fonts */
    @import url('https://fonts.googleapis.com/css2?family=Playfair+Display:wght@400;700&family=Open+Sans:wght@400;600&family=Source+Serif+4:ital,wght@0,400;0,600;1,400&display=swap');

    /* Root defaults (desktop / large screens) */
    html {
      font-family: 'Open Sans', sans-serif;
      font-size: 18px;   /* base size */
      line-height: 1.6;
      background-color: #fafafa;
      color: #333;
    }

    /* Mobile‑first adjustments */
    @media (max-width: 767px) {
      html { font-size: 15px; }            /* down‑scale all rem units */
      h1.publication-title { font-size: 2rem !important; }
      h2.title.is-3       { font-size: 1.5rem !important; }
      .content,
      .publication-authors,
      .publication-links a span,
      .link-block span    { font-size: 1rem; }
      figcaption          { font-size: 0.85rem; }
      pre code            { font-size: 0.9rem; }
      footer.footer       { font-size: 0.85rem; }
    }

    @media (max-width: 480px) {
      html { font-size: 14px; }            /* extra small screens */
    }

    body {
      background-color: white;
      margin: 0;
      padding: 0;
    }

    /* Headings */
    h1.publication-title,
    h2.title.is-3 {
      font-family: 'Playfair Display', serif !important;
    }

    h1.publication-title { font-size: 3rem !important; }
    h2.title.is-3       { font-size: 2rem !important; }

    /* Text blocks */
    .content,
    .publication-authors,
    .publication-links a span,
    .link-block span {
      font-size: 1.125rem;
      font-family: 'Open Sans', sans-serif;
    }

    /* Layout & components */
    .container { max-width: 720px; margin: 0 auto; padding: 0 1rem; }
    .hero .container { max-width: 960px; }
    .hero      { padding: 2.5rem 1rem; background-color: #f0f0f0; }
    .section   { padding: 3rem 0; }

    .teaser-image {
      max-width: 100%;
      height: auto;
      margin: 1.5rem auto;
      display: block;
      border-radius: 8px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }

    figcaption {
      font-size: 0.95rem;
      color: #555;
      margin-top: 0.5rem;
      text-align: center;
      font-style: italic;
      font-family: 'Open Sans', sans-serif;
    }

    pre {
      display: block;
      overflow-x: auto;
      padding: 1rem;
      background: #f5f5f5;
      border-radius: 4px;
      white-space: pre;
      word-break: normal;
    }

    pre code {
      font-family: monospace;
      font-size: 1rem;
      background: #f5f5f5;
      padding: 1rem;
      border-radius: 4px;
      overflow-x: auto;
    }

    footer.footer {
      background-color: #f8f8f8;
      font-size: 0.95rem;
      padding: 2rem 1rem;
    }
  </style>

  <!-- Scripts -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<!-- MathJax v3 (LaTeX rendering) -->
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<style>
  /* 轻量 tooltip：把说明放到 data-note，悬停时浮出 */
  .hover-note {
    position: relative;
    cursor: help;
    border-bottom: 1px dotted #999;
    display: inline-block;
    line-height: 1;
    padding: 0 0.2em;
    color: #666;
    font-size: 0.95em;
  }
  .hover-note::after {
    content: attr(data-note);
    position: absolute;
    left: 0;
    bottom: 125%;
    transform: translateY(-4px);
    background: #111;
    color: #fff;
    padding: 8px 10px;
    border-radius: 6px;
    box-shadow: 0 6px 18px rgba(0,0,0,0.2);
    white-space: normal;
    max-width: 420px;
    font-size: 0.9em;
    line-height: 1.4;
    opacity: 0;
    pointer-events: none;
    transition: opacity .15s ease, transform .15s ease;
    z-index: 1000;
  }
  .hover-note::before {
    content: '';
    position: absolute;
    left: 10px;
    bottom: 125%;
    transform: translateY(6px);
    border: 6px solid transparent;
    border-top-color: #111;
    opacity: 0;
    transition: opacity .15s ease;
    z-index: 1001;
  }
  .hover-note:hover::after,
  .hover-note:hover::before {
    opacity: 1;
    transform: translateY(0);
  }

  /* 可选：长公式块与提示图标的行内排版 */
  .formula-wrap {
    display: inline-flex;
    align-items: center;
    gap: 8px;
  }
</style>

</head>

<body>

  <!-- Hero / Title block -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <div class="columns is-centered">
          <div class="column has-text-centered">

            <!-- Title -->
            <h1 class="title is-1 publication-title">
              Shrinking the Variance: Shrinkage Baselines for Reinforcement Learning with Verifiable Rewards
            </h1>

            <!-- Authors -->
            <p class="is-size-5 publication-authors">
              <span class="author-block"><a href="https://scholar.google.com/citations?user=SU6ooAQAAAAJ">Guanning Zeng</a></span>,
              <span class="author-block"><a href="https://zhaoyizhou1123.github.io">Zhaoyi Zhou</a></span>,
              <span class="author-block"><a href="https://daman1209arora.github.io">Daman Arora</a></span>,
              <span class="author-block"><a href="https://azanette.com">Andrea Zanette</a></span>
            </p>
            <p class="is-size-5 publication-authors">
              Carnegie Mellon University
            </p>

            <!-- Links -->
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2506.09016" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"><i class="ai ai-arxiv"></i></span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/Zanette-Labs/speed-rl" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/collections/rqzhang/speed-rl-684a72dfb24ea72540c32fa1" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <svg class="svg-inline--fa fa-database fa-w-14" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="database" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentColor" d="M448 73.143v45.714C448 159.143 347.667 192 224 192S0 159.143 0 118.857V73.143C0 32.857 100.333 0 224 0s224 32.857 224 73.143zM448 176v102.857C448 319.143 347.667 352 224 352S0 319.143 0 278.857V176c48.125 33.143 136.208 48.572 224 48.572S399.874 209.143 448 176zm0 160v102.857C448 479.143 347.667 512 224 512S0 479.143 0 438.857V336c48.125 33.143 136.208 48.572 224 48.572S399.874 369.143 448 336z"></path></svg>
                  </span>
                  <span>Datasets</span>
                </a>
              </span>
            </div> <!-- /.publication-links -->

            <!-- Teaser sentence -->
            <p class="is-size-4 has-text-centered" style="margin-top: 2rem; font-style: italic; color: #555; font-family: 'Source Serif 4', Georgia, serif;">
              "Careful interpolation between prompt and batch means for baselines in RLVR reduces the policy gradient variance."
            </p>

          </div> <!-- /.column -->
        </div> <!-- /.columns -->
      </div> <!-- /.container -->
    </div> <!-- /.hero-body -->
  </section>

  <!-- Main content -->
  <section class="section">
    <div class="container is-max-widescreen">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <!-- Overview -->
          <div class="content has-text-justified">
            <p>
              We introduce <strong>James-Stein Shrinkage Baseline</strong>, a baseline for critic‑free, variance‑reduced reinforcement learning with verifiable rewards (RLVR) in large reasoning models. 
            </p>
            <ul>
              <li><strong>Lower‑variance updates.</strong> A closed‑form, data‑dependent shrinkage coefficient pools signal across the batch when per‑prompt estimates are noisy and backs off when prompts are heterogeneous.</li>
              <li><strong>Drop‑in & compute‑light.</strong> Works out of the box with rule‑based RLVR algorithms such as GRPO, RLOO, REINFORCE++, DAPO, DR.GRPO, GSPO and CISPO, without introducing any additional hyperparameters or computational overhead.</li>
              <li><strong>Better training stability.</strong> Outperforming existing baselines under different tasks, models and rollout budgets.</li>
            </ul>
            <figure>
              <img src="./static/images/teaser_01.png" alt="Overview of JSPO" class="teaser-image">
              <figcaption><strong>Figure 1:</strong> In critic-free RLVR, we can refine the common choice of using noisy prompt-mean baselines by using a James–Stein shrinkage baseline. The shrinkage adapts to within‑prompt noise and across‑prompt heterogeneity, yielding lower‑variance yet unbiased gradients and better, steadier training.</figcaption>
            </figure>
          </div> <!-- /.content -->

        <h2 class="title is-3" id="srt">Introduction</h2>
        <div class="content has-text-justified">
          
          <h3 class="title is-4" style="margin-top: 1.5rem; margin-bottom: 1rem;">Problem Setting</h3>
          <p>
            We consider Reinforcement Learning with Verifiable Rewards (RLVR) in large reasoning models. In this setting, we aim to optimize a language model policy <span style="white-space:nowrap;">\(\pi_\theta(y \mid x)\)</span>, where \(x\) is a prompt (e.g., a math problem) and \(y\) is the model's response (e.g., a reasoning chain and answer). The quality of each response is measured by a verifiable reward function \(r(x,y)\in\mathbb{R}\), which can be computed using, for instance, automatic graders that check correctness.
          </p>
          
          <p>
            At each reinforcement learning iteration, we follow a standard sampling procedure: we first sample a batch of \(n\) prompts \(x_1,\dots,x_n\) from the training distribution. Then, for each prompt \(x_i\), we generate \(m\) independent responses \(y_i^1,\dots,y_i^m\) by sampling from the current policy. Each response \(y_i^j\) receives a corresponding reward \(r_i^j := r(x_i,y_i^j)\). This produces a total of \(n \times m\) prompt-response pairs with associated rewards per iteration.
          </p>

          <h3 class="title is-4" style="margin-top: 1.5rem; margin-bottom: 1rem;">GRPO Objective</h3>
          <p>
            The surrogate objective in the well-known Group Relative Policy Optimization (GRPO) algorithm can be written as:
          </p>

          <p class="formula-wrap">
            \[
            \mathcal{J}_{\mathrm{GRPO}}(\theta)
            := \frac{1}{n}\sum_{i=1}^{n}\frac{1}{m}\sum_{j=1}^{m}\frac{1}{\lvert y_i^{\,j}\rvert}\sum_{t=1}^{\lvert y_i^{\,j}\rvert}
            \min\!\Big(\,\rho_{i,t}^{\,j}\,A_i^{\,j},\;
            \operatorname{clip}\!\big(\rho_{i,t}^{\,j},\,1-\epsilon,\,1+\epsilon\big)\,A_i^{\,j}\Big) \, ,
            \]
          </p>
          
          <p>
            where \(\lvert y_i^{\,j}\rvert\) denotes the token length of response \(y_i^{\,j}\), and the objective aggregates contributions across all prompts, responses, and tokens.
          </p>

          <p>
            Here, \(\rho_{i,t}^{\,j}\) is the <strong>importance sampling ratio</strong> at token position \(t\):
          </p>

          <p class="formula-wrap">
            \[
            \rho_{i,t}^{\,j}
            := \frac{\pi_\theta\!\big(y_{i,t}^{\,j}\mid x_i,\,y_{i,1{:}t-1}^{\,j}\big)}
            {\pi_{\theta_{\mathrm{old}}}\!\big(y_{i,t}^{\,j}\mid x_i,\,y_{i,1{:}t-1}^{\,j}\big)} \, ,
            \]
          </p>

          <p>
            which measures the ratio between the current policy \(\pi_\theta\) and the old policy \(\pi_{\theta_{\mathrm{old}}}\) (the behavior policy from the previous iteration) for generating token \(y_{i,t}^{\,j}\) given the prompt \(x_i\) and the preceding tokens \(y_{i,1{:}t-1}^{\,j}\). 
          </p>

          <p>
            Meanwhile, \(A_i^{\,j}\) is the <strong>advantage</strong>, defined as:
          </p>

          <p class="formula-wrap">
            \[
            A_i^{\,j} := r_i^{\,j} - b_i^{\,j}\, ,
            \]
          </p>

          <p>
            where \(r_i^{\,j}\) is the observed reward for response \(y_i^{\,j}\), and \(b_i^{\,j}\) is the <strong>baseline</strong>. The advantage captures how much better (or worse) a particular response is compared to the baseline. By subtracting the baseline from the raw reward, we center the signal and reduce the variance of gradient estimates, which is crucial for stable and efficient learning.
          </p>

          <p>
            Note that in standard GRPO implementations, the advantage \(A_i^{\,j}\) is often further normalized by dividing by the empirical standard deviation across all advantages in the batch. However, we omit this normalization step in our formulation, as recent empirical work has demonstrated that both the normalized and unnormalized versions exhibit similar scaling behavior in terms of final performance and training dynamics.
          </p>

          <p>
            We focus on \(b^j_i\), usually referred to the baseline in deep reinforcement learning. Subtracting the rewards \(r_i^j\) by the baseline reduces the variance of the policy-gradient estimate, which is very essential to stabilize training. For example, in GRPO, the baseline is the empirical prompt mean. Below is a leave-one-out estimation:
          </p>

          <p class="has-text-centered">
            \(b_i^j = \frac{1}{m-1} \sum_{k=1}^{m} r_i^k, \; k\neq j\)
          </p>

          <!-- 结尾问题，按指定字体与样式 -->
          <p class="is-size-5 has-text-centered" style="font-style: italic; color: #555; font-family: 'Source Serif 4', Georgia, serif;">
            However, is the empirical prompt mean the theoretically optimal baseline for RLVR?
          </p>
        </div>

          <h2 class="title is-3" id="srt">A Bias-Variance Tradeoff</h2>
          <div class="content has-text-justified">          
            <p>
              A standard and accurate simplification for baseline estimation \(b(x)\) is to make it close to the <em>value‑function</em> \(\mu(x):=\mathbb{E}_{y\sim\pi_\theta}[r(x,y)]\) (e.g. In all actor-critic methods, the value function is estimated from a critic network which serves as the baseline). For a fixed prompt \(x\),
            </p>
            <p>
              \[
              \mathbb{E}_{y}\!\big[(r(x,y)-b(x))^2\big]
              \;=\;
              \underbrace{\mathrm{Var}_{y}[r(x,y)]}_{\text{irreducible}} \;+\;
              \underbrace{(b(x)-\mu(x))^2}_{\text{baseline MSE}}\,,
              \]
            </p>
            <p>
              so variance reduction of policy gradient estimator reduces to estimating the value function \(\mu(x)\) accurately. In the multi‑prompt setting, the target is the <em>batch</em> MSE over all prompts and rollouts:
            </p>
            <p> \[ \begin{aligned} \mathrm{MSE} &= \frac{1}{mn}\sum_{i=1}^{n}\sum_{j=1}^{m} \mathbb{E}\!\left[\bigl(b_i^j-\mu_i\bigr)^2\right] \\[2pt] &= \frac{1}{mn}\sum_{i=1}^{n}\sum_{j=1}^{m} \mathbb{E}\!\left[\bigl(b_i^j-\mathbb{E}[b_i^j]+\mathbb{E}[b_i^j]-\mu_i\bigr)^2\right] \\[2pt] &= \frac{1}{mn}\sum_{i=1}^{n}\sum_{j=1}^{m} \Bigl(\mathbb{E}\bigl[(b_i^j-\mathbb{E}[b_i^j])^2\bigr] + 2\,\mathbb{E}[b_i^j-\mathbb{E}[b_i^j]]\,(\mathbb{E}[b_i^j]-\mu_i) + (\mathbb{E}[b_i^j]-\mu_i)^2\Bigr) \\[2pt] &= \frac{1}{mn}\sum_{i=1}^{n}\sum_{j=1}^{m} \Bigl(\underbrace{\mathbb{V}\mathrm{ar}[\,b_i^j\,]}_{\text{Variance}} + \underbrace{(\mathbb{E}[b_i^j]-\mu_i)^2}_{\text{Bias}^2}\Bigr). \end{aligned} \] </p>
          
            <p>
              This batch objective reveals a clear <strong>variance–bias trade‑off</strong> in baseline design:
            </p>
            <ul>
              <li><strong>Prompt means (e.g., GRPO/RLOO):</strong> set \(b_i^j\) near the prompt’s empirical mean, which is the usual choice for most RLVR algorithms. This minimizes bias to \(\mu_i\) but can be <em>high variance</em> when \(m\) is small.</li>
              <li><strong>Batch means (e.g., REINFORCE++):</strong> is another possible choice. It average across prompts to reduce variance, but this introduces <em>bias</em> when prompts’ true values \(\mu_i\) differ.</li>
            </ul>
          
            <p>
              Because we must estimate <em>many</em> prompt values simultaneously, neither extreme is optimal in terms of MSE from the whole batch level. 
            </p>
            <p class="is-size-5 has-text-centered" style="font-style: italic; color: #555; font-family: 'Source Serif 4', Georgia, serif;">
              How can we create a baseline estimator which enjoys lowest MSE with the ground truth value function? 
            </p>            
          </div>
          
          <!-- James–Stein Policy Optimization (JSPO): Algorithm Design -->
          <h2 class="title is-3" id="algo">Shrinkage Baseline</h2>
          <div class="content has-text-justified">
            <p>
              Based on the solution to the famous Stein's paradox in statistics, an adaptive interpolation between prompt mean and batch mean form a strictly better estimation during simultaneous estimation of multiple value functions.
            </p>
            <p>
              \[
              b_{ij}^{\text{JS}} \;=\; (1-\hat{\lambda}_i)\,\hat{\mu}_{i}^{-j} \;+\; \hat{\lambda}_i\,\hat{\mu}_{-i},
              \]
            </p>
            <p>
              where
            </p>
            <p>
              \[
              \hat{\mu}_{i}^{-j} \;=\; \frac{1}{m-1}\!\!\sum_{j'\neq j} r_i^{\,j'}, 
              \qquad
              \hat{\mu}_{-i} \;=\; \frac{1}{n-1}\!\!\sum_{k\neq i} \hat{\mu}_k,
              \quad
              \hat{\mu}_k \;=\; \frac{1}{m}\sum_{j=1}^{m} r_k^{\,j}.
              \]
            </p>
            <p>
              The shrinkage coefficient \(\hat{\lambda}_i\) is chosen to minimize the batch MSE of value estimates. Through derivations, we have 
            </p>
            <p>
              \[
              (\lambda_i^\ast)\;=\;\frac{n-1}{n}\cdot \frac{v^2}{\,s^2+v^2\,},
              \qquad
              v^2 \;=\; \frac{1}{m-1}\,\mathbb{E}_{x}\!\big[\sigma^2(x)\big],\quad
              s^2 \;=\; \mathrm{Var}_{x}\!\big[\mu(x)\big],
              \]
              with \(\sigma^2(x)=\mathrm{Var}_{y\sim\pi_\theta(\cdot|x)}[r(x,y)]\) and \(\mu(x)=\mathbb{E}_{y\sim\pi_\theta(\cdot|x)}[r(x,y)]\).
            </p>
            <p>
              In practice, we can estimate both components from the current batch using plug‑in statistics:
            </p>
            <p>
              \[
              \hat{v}_{-i} \;=\; \frac{1}{n-1}\sum_{k\neq i}\left[\frac{1}{m(m-1)}\sum_{j=1}^{m}\big(r_k^{\,j}-\hat{\mu}_k\big)^2\right],
              \qquad
              \hat{s}_{-i} \;=\; \frac{1}{n-1}\sum_{k\neq i}\big(\hat{\mu}_k-\hat{\mu}_{-i}\big)^2,
              \]
              yielding the per‑prompt shrinkage coefficient
              \[
              \hat{\lambda}_i \;=\; \frac{n-1}{n}\cdot \frac{\hat{v}_{-i}}{\,\hat{v}_{-i}+\hat{s}_{-i}\,}.
              \]
              Substituting into the baseline gives \(b_{ij}^{\text{JS}}\) above. 
            </p>
            <p>
              when \(v^2 \gg s^2\) (e.g. few rollouts, high within‑prompt noise), \(\hat{\lambda}_i\) is large and our James-Stein baseline adaptively leaning towards the batch mean, similar to REINFORCE++; when \(s^2 \gg v^2\) (e.g. sufficient rollouts, heterogeneous training data), \(\hat{\lambda}_i \approx 0\) and the James-Stein baseline reduces to the prompt mean, similar to GRPO. Under arbitrary conditions, the James-Stein baseline is theoretically guaranteed to be a more accurate compared with other baselines.
            </p>
          </div>          


          <!-- Experiments -->
          <h2 class="title is-3" id="experiments">Experiments</h2>
          <div class="content has-text-justified">
            <p>
              We demonstrate SPEED's efficacy by comparing the <strong>wall-clock time</strong> to achieve certain target performance for baseline RL algorithms and for SPEED variants.
            </p>
            <ul>
              <li>Baseline: <a href="https://arxiv.org/abs/2402.14740" target="_blank" rel="noopener noreferrer">RLOO</a> and <a href="https://arxiv.org/abs/2503.14476" target="_blank" rel="noopener noreferrer">DAPO</a>. We compare RLOO vs SPEED-RLOO, and DAPO vs SPEED-DAPO.</li>
              <li>Training Dataset: <a href="https://huggingface.co/datasets/agentica-org/DeepScaleR-Preview-Dataset" target="_blank" rel="noopener noreferrer">DeepScaleR</a>, <a href="https://huggingface.co/datasets/AI-MO/NuminaMath-TIR" target="_blank" rel="noopener noreferrer">NuminaMath</a>, and <a href="https://huggingface.co/datasets/BytedTsinghua-SIA/DAPO-Math-17k" target="_blank" rel="noopener noreferrer">DAPO-17k</a> (1k held-out set excluded)</li>
              <li>Benchmarks: 1k held-out set from DAPO-17k, MATH500, AMC23, AIME2024, and AIME 2025. See <a href="https://huggingface.co/collections/rqzhang/speed-rl-684a72dfb24ea72540c32fa1" target="_blank" rel="noopener noreferrer">here</a>.</li>
            </ul>
            <p>
              Empirically, SPEED variants substantially accelerate the training compared to the baseline methods by <strong>2x - 6x times</strong>. For detailed empirical results, please refer to our full paper.
            </p>
            <figure>
              <img src="./static/images/main_figure.png" alt="Main experimental figure" class="teaser-image">
              <figcaption><strong>Figure 3:</strong> Validation accuracy on various mathematical reasoning benchmarks for SPEED variants of RL algorithms and base RL algorithms. Top: RLOO versus SPEED-RLOO; bottom: DAPO versus SPEED-DAPO. The initial model used is Qwen2.5-Math-7B, trained on the DeepScaleR dataset.</figcaption>
            </figure>
          </div> <!-- /.content -->


          <h2 class="title is-3" id="analysis">Analysis</h2>
          <div class="content has-text-justified">
            <p>
              We focus on providing insights into two key training dynamics that reveal the advantage of JSPO: namely, the <strong>adaptive shrinkage</strong> of the James–Stein coefficient and the <strong>reduced variance</strong> of the policy
              gradient.
            </p>
            <p>
              <strong>Adaptive Shrinkage.</strong> The shrinkage coefficient \(\lambda_i\) adapts to number of rollouts, heterogeneity of prompts, and the training progress.
            </p>
            <figure>
              <img src="./static/images/adaptive_shrinkage.png" alt="Adaptive shrinkage coefficient over training and rollout counts" class="teaser-image" style="width: 50%;">
              <figcaption>
                <strong>Figure 2:</strong> Moving‑average shrinkage coefficient \(\lambda\) during training with 2/4/8 rollouts.
                \(\lambda\) decreases with more rollouts and decays over time as the policy’s within‑prompt variance shrinks,
                illustrating JSPO’s data‑driven adaptation. 
              </figcaption>
            </figure>

            <p>
              <strong>Variance Reduction.</strong> JSPO’s goal is to lower the variance of the vector‑valued policy‑gradient
              estimator \(g(x,Y;\theta)\). During training, we track the (unbiased) scalar summary \(\mathrm{Var}[g]\) via micro‑batch
              gradients:
            </p>
            <p>
              \[
                \widehat{\mathrm{Var}}(g)
                \;=\;\frac{1}{m}\cdot\frac{1}{m-1}\sum_{i=1}^{m}\bigl\|g_i-\bar g\bigr\|_2^2
                \;=\;\frac{1}{m}\cdot\frac{1}{m-1}\!\left(\sum_{i=1}^{m}\|g_i\|_2^2 - m\,\|\bar g\|_2^2\right),
                \quad \bar g=\frac{1}{m}\sum_{i=1}^{m} g_i.
              \]
              Empirically, JSPO reduces the running average of gradient variance relative to RLOO by
              <strong>11.2%</strong>, <strong>17.4%</strong>, <strong>31.6%</strong>, and <strong>67.1%</strong> across different models and setups. 
            </p>

            <figure>
              <img src="./static/images/variance_reduction.png" alt="Estimated policy‑gradient variance during training: JSPO vs. RLOO" class="teaser-image", style="width: 90%;">
              <figcaption>
                <strong>Figure 3:</strong> Estimated variance of the policy gradient during training (lower is better). With the
                James–Stein baseline, variance consistently drops across models and tasks, yielding steadier RL updates.
              </figcaption>
            </figure>

            <h2 class="title is-3" id="srt">Citation</h2>
            <p>If you find this work useful, please cite it as follows:</p>

            <pre class="box" style="white-space: pre-wrap; word-break: break-word; overflow-x: auto; font-family: monospace; font-size: 0.9rem;">
              @misc{zhang2025speedrlfastertrainingreasoning,
                title={SPEED-RL: Faster Training of Reasoning Models via Online Curriculum Learning}, 
                author={Ruiqi Zhang and Daman Arora and Song Mei and Andrea Zanette},
                year={2025},
                eprint={2506.09016},
                archivePrefix={arXiv},
                primaryClass={cs.LG},
                url={https://arxiv.org/abs/2506.09016}
              }
              </pre>
              


  <br><br>
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <p>
        Corresponding Authors: <a href="mailto:zanette@cmu.edu">Andrea Zanette</a>, <a href="mailto:zgn21@mails.tsinghua.edu.cn">Guanning Zeng</a>.<br>
      </p>
    </div>
  </div>
</footer>

</body>
</html>
